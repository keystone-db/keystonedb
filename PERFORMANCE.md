# KeystoneDB Performance Guide

This document provides performance characteristics, optimization techniques, and best practices for KeystoneDB.

## Table of Contents

1. [Performance Characteristics](#performance-characteristics)
2. [Best Practices](#best-practices)
3. [Optimization Techniques](#optimization-techniques)
4. [Monitoring and Tuning](#monitoring-and-tuning)
5. [Benchmarking](#benchmarking)

## Performance Characteristics

### Write Performance

**Throughput:** ~10-50k operations/second

**Factors:**
- **Memtable flush frequency** (default: 1000 records per stripe)
- **WAL fsync overhead** (durability vs performance trade-off)
- **Group commit effectiveness** (more concurrent writes = better batching)
- **Stripe distribution** (256 stripes enable parallel writes)

**Characteristics:**
- ‚úÖ **Fast writes**: O(log n) memtable insert + sequential WAL append
- ‚úÖ **No read amplification**: Writes don't require reading existing data
- ‚ö†Ô∏è **Write amplification**: Compaction rewrites data (typical for LSM trees)
- ‚ö†Ô∏è **Flush pauses**: Brief stalls when memtable flushes to SST

**Write Latency Breakdown:**
```
Total: ~100-500Œºs per write
‚îú‚îÄ Memtable insert: 5-10Œºs (in-memory BTreeMap)
‚îú‚îÄ WAL append: 50-200Œºs (disk write + fsync)
‚îî‚îÄ Lock acquisition: <5Œºs (usually uncontended)
```

### Read Performance

**Throughput:**
- **Hot data (memtable):** ~100k+ ops/second
- **Cold data (SST):** ~10k ops/second

**Factors:**
- **Data location**: Memtable (fast) vs SST (slower)
- **SST count**: More SSTs = more files to check
- **Bloom filter effectiveness**: Reduces unnecessary disk reads
- **Compaction state**: Fewer SSTs after compaction = faster reads

**Characteristics:**
- ‚úÖ **Fast memtable reads**: O(log n) in-memory lookup
- ‚úÖ **Bloom filter optimization**: Skips SSTs that don't contain key
- ‚ö†Ô∏è **Read amplification**: May need to check multiple SSTs
- ‚ö†Ô∏è **Cache cold starts**: First read of SST requires disk I/O

**Read Latency Breakdown:**
```
Memtable hit: ~10-50Œºs
‚îú‚îÄ Acquire read lock: <5Œºs
‚îî‚îÄ BTreeMap lookup: 5-45Œºs

SST hit: ~100-1000Œºs
‚îú‚îÄ Bloom filter check: 1-5Œºs per SST
‚îú‚îÄ Binary search: 10-100Œºs per SST
‚îî‚îÄ Disk I/O: 100-1000Œºs (mmap or read)
```

### Query Performance

**Partition Key Query (Efficient):**
- **Throughput:** ~10-50k ops/second
- **Pattern:** `Query::new().partition_key(b"user#123")`
- **Why fast:** Direct stripe selection, range scan within stripe

**Sort Key Range Query:**
- **Throughput:** ~5-20k ops/second (depends on range size)
- **Pattern:** `Query::new().partition_key(pk).sort_key_between(a, b)`
- **Why slower:** Must scan range in memtable + all SSTs

**Scan (Full Table, Slow):**
- **Throughput:** ~1-5k ops/second (highly variable)
- **Pattern:** `ScanBuilder::new().build()`
- **Why slow:** Must scan all 256 stripes, all SSTs
- **Mitigation:** Use parallel scan for large tables

**Performance Comparison:**
```
Operation                    Relative Performance
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Get by pk                    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Fastest
Query by pk + sk             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë Fast
Query by pk (range)          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë Moderate
Scan with filter             ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Slow
Scan (full table)            ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Slowest
```

### Scan Performance

**Sequential Scan:**
- **Single-threaded:** ~1-5k items/second
- **256 stripes scanned sequentially**

**Parallel Scan:**
- **Multi-threaded:** ~5-20k items/second
- **Up to 256 segments** (1 segment per stripe)
- **Linear scalability** with segment count

**Example Parallel Scan:**
```rust
// Segment 0 of 4 (processes stripes 0, 4, 8, ...)
let scan1 = ScanBuilder::new().parallel(4, 0).build();

// Segment 1 of 4 (processes stripes 1, 5, 9, ...)
let scan2 = ScanBuilder::new().parallel(4, 1).build();

// Run in parallel
let (result1, result2) = rayon::join(
    || db.scan(scan1),
    || db.scan(scan2),
);
```

### Compaction Impact

**During Compaction:**
- **Write throughput:** ~70-90% of normal (compaction uses write lock)
- **Read throughput:** Mostly unaffected (uses read lock)
- **Disk I/O:** Increases significantly
- **Latency:** May see occasional spikes (100-500ms)

**After Compaction:**
- **Read throughput:** +20-50% improvement (fewer SSTs to scan)
- **Write throughput:** Returns to normal
- **Disk space:** Reduced (tombstones and duplicates removed)

## Best Practices

### 1. Always Use Partition Key for Queries

**‚ùå Avoid:**
```rust
// Scan entire table (SLOW)
let scan = ScanBuilder::new()
    .filter_expression("email = :email")
    .expression_value(":email", "alice@example.com")
    .build();
```

**‚úÖ Prefer:**
```rust
// Query by partition key (FAST)
let query = Query::new()
    .partition_key(b"user#123");
```

**Why:** Queries with partition key go directly to the correct stripe and use efficient range scans. Scans must check all 256 stripes.

### 2. Use Indexes for Non-Key Queries

**Scenario:** Need to query users by email address

**‚ùå Avoid:**
```rust
// Full table scan (SLOW)
let scan = ScanBuilder::new()
    .filter_expression("email = :email")
    .expression_value(":email", "alice@example.com")
    .build();
```

**‚úÖ Better:**
```rust
// Create GSI on email
let db = Database::create_with_options(path, options)?;

// Add GSI configuration in schema
// gsi_pk = email, gsi_sk = timestamp

// Query GSI (FAST)
let query = Query::new()
    .partition_key(b"alice@example.com")
    .index_name("by-email");
```

**Why:** GSI allows efficient queries by non-key attributes. Trade-off is additional storage and write overhead.

### 3. Use LIMIT to Reduce Data Transfer

**‚ùå Inefficient:**
```rust
// Fetch all results, process only 10
let response = db.scan(ScanBuilder::new().build())?;
let first_10: Vec<_> = response.items.into_iter().take(10).collect();
```

**‚úÖ Efficient:**
```rust
// Fetch only 10 items
let response = db.scan(
    ScanBuilder::new()
        .limit(10)
        .build()
)?;
```

**Why:** LIMIT reduces I/O, memory usage, and network transfer (if remote). Storage engine can short-circuit after finding N items.

### 4. Use SELECT Projection

**‚ùå Wasteful:**
```sql
-- Fetch all attributes, use only name and email
SELECT * FROM users WHERE pk = 'user#123'
```

**‚úÖ Efficient:**
```sql
-- Fetch only needed attributes
SELECT name, email FROM users WHERE pk = 'user#123'
```

**Why:** Reduces memory allocation, serialization overhead, and network transfer. Especially important for items with large attributes.

### 5. Batch Operations for Bulk Writes

**‚ùå Slow:**
```rust
// Individual writes (many fsync calls)
for item in items {
    db.put(item.key, item.value)?;
}
```

**‚úÖ Fast:**
```rust
// Batch write (fewer fsync calls)
let mut batch = db.batch_write();
for item in items {
    batch = batch.put(item.key, item.value);
}
batch.execute()?;
```

**Why:** Batch writes enable group commit, reducing fsync overhead from N calls to 1-2 calls.

**Performance Gain:** 5-10x improvement for bulk inserts

### 6. Use Transactions for Atomicity

**Use Case:** Multi-item operations that must succeed or fail together

```rust
// All-or-nothing write
db.transact_write()
    .put(b"user#123", user_item)
    .put(b"account#456", account_item)
    .update(b"counter#1", "SET count = count + 1", None)
    .execute()?;
```

**Why:** Transactions ensure consistency without application-level rollback logic. Slight overhead (~10-20%) compared to individual writes.

### 7. Parallel Scan for Large Tables

**‚ùå Slow:**
```rust
// Single-threaded scan
let response = db.scan(ScanBuilder::new().build())?;
```

**‚úÖ Fast:**
```rust
use rayon::prelude::*;

// Parallel scan with 4 segments
let results: Vec<_> = (0..4).into_par_iter().map(|segment| {
    let scan = ScanBuilder::new()
        .parallel(4, segment)
        .build();
    db.scan(scan).unwrap()
}).collect();

// Merge results
let all_items: Vec<_> = results.into_iter()
    .flat_map(|r| r.items)
    .collect();
```

**Performance Gain:** Near-linear scaling (4 segments ‚âà 4x faster)

### 8. Design Keys for Query Patterns

**Good Key Design:**
```
pk = "user#<user_id>"          # User entity
pk = "user#<user_id>", sk = "profile"      # User profile
pk = "user#<user_id>", sk = "post#<timestamp>"  # User posts
```

**Why:**
- Same partition key groups related items
- Sort key enables range queries (e.g., posts in date range)
- Efficient queries without scans

**Bad Key Design:**
```
pk = "user", sk = "<user_id>"   # All users in one partition (hotspot!)
pk = "<random_uuid>"            # No query pattern, requires full scan
```

## Optimization Techniques

### 1. Tune Memtable Flush Threshold

**Default:** 1000 records per stripe

**Configuration:**
```rust
// In kstone-core/src/lsm.rs
const MEMTABLE_THRESHOLD: usize = 1000;  // Increase for better write throughput
```

**Trade-offs:**

| Threshold | Write Throughput | Memory Usage | Recovery Time |
|-----------|------------------|--------------|---------------|
| 500       | Lower            | Low          | Fast          |
| 1000      | Medium (default) | Medium       | Medium        |
| 5000      | Higher           | High         | Slow          |
| 10000     | Highest          | Very High    | Very Slow     |

**Recommendation:**
- **Write-heavy:** Increase to 5000 or 10000
- **Memory-constrained:** Decrease to 500
- **Balanced:** Keep default (1000)

### 2. Adjust Compaction Frequency

**Default:** Check every 5 seconds, compact if ‚â•4 SSTs

**Configuration:**
```rust
// In kstone-core/src/compaction.rs
const COMPACTION_CHECK_INTERVAL: Duration = Duration::from_secs(5);
const MIN_SST_COUNT_FOR_COMPACTION: usize = 4;
```

**Trade-offs:**

| Strategy | Read Perf | Write Perf | Space Usage |
|----------|-----------|------------|-------------|
| Aggressive (‚â•2 SSTs) | High | Lower | Low |
| Moderate (‚â•4 SSTs, default) | Medium | Medium | Medium |
| Lazy (‚â•10 SSTs) | Lower | Higher | High |

**Recommendation:**
- **Read-heavy:** Aggressive (‚â•2 SSTs)
- **Write-heavy:** Lazy (‚â•10 SSTs)
- **Balanced:** Moderate (default)

### 3. Bloom Filter Tuning

**Default:** ~1% false positive rate

**Configuration:**
```rust
// In kstone-core/src/bloom.rs
let bloom = BloomFilter::new(record_count, 0.01);  // 1% FPR
```

**Trade-offs:**

| False Positive Rate | Memory per 1000 keys | Unnecessary Disk Reads |
|---------------------|----------------------|------------------------|
| 0.1% (aggressive)   | ~1.8 KB              | 1 per 1000 lookups     |
| 1% (default)        | ~1.2 KB              | 10 per 1000 lookups    |
| 5% (relaxed)        | ~0.8 KB              | 50 per 1000 lookups    |

**Recommendation:**
- **Memory-constrained:** 5% FPR
- **Read-optimized:** 0.1% FPR
- **Balanced:** 1% FPR (default)

### 4. Memory-Mapped File I/O

**Current:** Memory-mapped reads for SST files

**Benefits:**
- OS page cache automatically caches hot data
- Avoids read() system call overhead
- Zero-copy reads

**Considerations:**
- Memory usage appears high (virtual memory)
- Actual memory usage determined by OS page cache
- Works best with sufficient RAM

### 5. WAL Group Commit

**Automatic:** Multiple writes share single fsync

**To maximize benefit:**
```rust
// Write in batches from multiple threads
use rayon::prelude::*;

items.par_iter().for_each(|item| {
    db.put(item.key, item.value).unwrap();
});
```

**Why:** Concurrent writes increase chance of group commit, reducing fsync calls from N to ~sqrt(N).

## Monitoring and Tuning

### Key Metrics to Monitor

#### 1. SST File Count per Stripe

**Check:**
```bash
# Count SST files for stripe 0
ls mydb.keystone/000_*.sst | wc -l
```

**Interpretation:**
- **0-3 SSTs:** Healthy, no action needed
- **4-10 SSTs:** Normal, compaction running
- **10+ SSTs:** Compaction falling behind, may need tuning

**Action:**
- Increase compaction frequency
- Reduce memtable flush threshold
- Investigate write rate spike

#### 2. Database Size

**Check:**
```bash
du -sh mydb.keystone
```

**Interpretation:**
- **Size growth:** Normal with writes
- **Sudden spike:** Check for tombstone accumulation
- **Not shrinking after deletes:** Compaction not running

**Action:**
- Force compaction manually (if implemented)
- Check compaction manager is running

#### 3. Query vs Scan Ratio

**Measure in application:**
```rust
let query_count = counter.query.load(Ordering::Relaxed);
let scan_count = counter.scan.load(Ordering::Relaxed);
let ratio = scan_count as f64 / query_count as f64;
```

**Interpretation:**
- **Ratio < 0.1:** Excellent (mostly queries)
- **Ratio 0.1-0.5:** Good (some scans)
- **Ratio > 0.5:** Poor (too many scans)

**Action:**
- Add indexes (GSI) for common scan patterns
- Redesign access patterns to use partition keys

#### 4. Write Latency P99

**Measure:**
```rust
let start = std::time::Instant::now();
db.put(key, item)?;
let duration = start.elapsed();
```

**Healthy Values:**
- **P50:** 100-300Œºs
- **P99:** 500-2000Œºs
- **P99.9:** 2-10ms

**Concerning Values:**
- **P99 > 10ms:** Potential WAL fsync issues
- **Spikes to 100ms+:** Likely compaction pauses

### Tuning Workflow

1. **Identify bottleneck:**
   - High write latency? ‚Üí Tune memtable threshold
   - High read latency? ‚Üí Check SST count, add indexes
   - High memory usage? ‚Üí Reduce memtable threshold

2. **Make one change at a time**
3. **Measure impact** (before/after comparison)
4. **Iterate**

## Benchmarking

### Simple Benchmark Template

```rust
use std::time::Instant;

fn benchmark_writes(db: &Database, count: usize) {
    let start = Instant::now();

    for i in 0..count {
        let key = format!("key{:010}", i);
        let item = ItemBuilder::new()
            .string("value", format!("data{}", i))
            .build();
        db.put(key.as_bytes(), item).unwrap();
    }

    let duration = start.elapsed();
    let ops_per_sec = count as f64 / duration.as_secs_f64();

    println!("Writes: {:.0} ops/sec", ops_per_sec);
}

fn benchmark_reads(db: &Database, count: usize) {
    let start = Instant::now();

    for i in 0..count {
        let key = format!("key{:010}", i);
        db.get(key.as_bytes()).unwrap();
    }

    let duration = start.elapsed();
    let ops_per_sec = count as f64 / duration.as_secs_f64();

    println!("Reads: {:.0} ops/sec", ops_per_sec);
}

fn benchmark_queries(db: &Database, count: usize) {
    let start = Instant::now();

    for i in 0..count {
        let key = format!("key{:010}", i);
        let query = Query::new().partition_key(key.as_bytes());
        db.query(query).unwrap();
    }

    let duration = start.elapsed();
    let ops_per_sec = count as f64 / duration.as_secs_f64();

    println!("Queries: {:.0} ops/sec", ops_per_sec);
}
```

### Expected Results (Reference Hardware)

**Hardware:** Modern laptop (SSD, 16GB RAM, 4 cores)

```
Operation          Throughput        P50 Latency    P99 Latency
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Put                20k ops/sec       50Œºs           500Œºs
Get (memtable)     100k ops/sec      10Œºs           50Œºs
Get (SST)          15k ops/sec       50Œºs           1ms
Query (pk)         25k ops/sec       40Œºs           400Œºs
Scan (full)        2k ops/sec        500ms          2s
Batch Write (100)  5k batches/sec    20ms           100ms
```

**Note:** Results vary significantly based on:
- Disk type (NVMe SSD > SATA SSD > HDD)
- Memory size (affects page cache hit rate)
- CPU speed (affects compression, hashing)
- Workload (hot vs cold data)

## Summary

### Key Takeaways

1. **Use partition keys** for all queries when possible
2. **Add indexes (GSI)** for non-key attribute queries
3. **Batch writes** for bulk operations
4. **Use LIMIT and projection** to reduce data transfer
5. **Parallel scan** for large table scans
6. **Monitor SST count** as indicator of compaction health
7. **Design keys** to match query patterns

### Performance Hierarchy

```
Fastest ‚Üí
    Get by partition key (memtable)
    Query by partition key
    Get by partition key (SST)
    Query by partition key + sort key range
    Scan with filter (indexed attributes)
    Scan with filter (non-indexed)
    Full table scan
‚Üê Slowest
```

### When to Optimize

- ‚úÖ **Optimize when:** P99 latency > 10ms, throughput < 1k ops/sec
- ‚ùå **Don't optimize when:** Performance meets requirements
- üéØ **Focus on:** Query patterns first, then tuning parameters

For more details on internal architecture, see [ARCHITECTURE.md](ARCHITECTURE.md).
